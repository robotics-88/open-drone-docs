{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Open Drone Stack","text":"<p>Welcome to the Open Drone Stack \u2014 a modular, ROS2-based system for autonomous drones. The full stack includes not only the ROS code to provide drone autonomy, but also a basic frontend and drone&lt;--&gt;server connection. Follow the complete Setup Guide to use our stack as is, or jump to Development to learn how to set custom missions and more for your application!</p>"},{"location":"#use-cases","title":"Use Cases","text":"<p>This system is designed for versatility, from research labs to commercial deployments. Our goal is to make advanced autonomy accessible: even a novice roboticist should be able to go from a newly built drone to custom autonomous missions in under an hour (and by custom, we mean a little to a lot more than a basic lawnmower pattern).</p>"},{"location":"#example-missions","title":"Example MissionsGoal PointPowerline FollowingTrail FollowingSmart Lawnmower","text":"\ud83d\udd25 <p>This custom mission is the most basic: Navigate to a goal point dropped on the map.</p> \u2192 Learn more \ud83d\udee0\ufe0f <p>Uses LiDAR to identify and follow a powerline, producing a vegetation encroachment map.</p> \u2192 Learn more \ud83d\udee0\ufe0f <p>This custom mission enables a new flight pattern in which the drone identifies and follows trails.</p> \u2192 Learn more \ud83c\udf32 <p>This custom mission uses our path manager's 'adaptive' lawnmower flight pattern, which adjusts waypoints based on user-defined decision criteria. E.g., add a new criteria to push the flight path toward species of interest.</p> \u2192 Learn more"},{"location":"#overview","title":"Overview","text":"<p>The Open Drone Stack is composed of:</p> <ul> <li>A ROS 2-based flight stack</li> <li>A REST API server on drone</li> <li>A web-based frontend</li> <li>A set of modular ROS 2 packages</li> <li>Optional simulator for SITL testing</li> </ul>"},{"location":"#primary-repos","title":"Primary Repos","text":"<ul> <li><code>open-drone-core</code>: main point of access for ROS2 flight stack</li> <li><code>open-drone-server</code>: provides a RESTful API on the drone</li> <li><code>open-drone-frontend</code>: web app, map-based user interface</li> </ul>"},{"location":"#ros2-flight-stack","title":"ROS2 Flight Stack","text":"<p>The 2 most critical repos are task-manager and vehicle-launch. Everything else in the stack is technically optional, but these are required for basic node and mission startup. However, to make the most use of the stack, we recommend at minimum the following:</p> <ul> <li>task-manager: state machine, receives and processes mission, manages flight controller params<ul> <li>This is where you add new missions</li> </ul> </li> <li>vehicle-launch: launch and vehicle config management<ul> <li>This is where you set up new vehicle configs or change perception modules</li> </ul> </li> <li>path_manager: takes task manager mission outputs and converts those goals to executable MAVROS setpoints (can be used with or without SLAM &amp;path planner)<ul> <li>This is where you get to decide how smart the drone really is, at minimum by enabling path planning, up to enabling full decision-making about what constitutes a good destination</li> </ul> </li> <li>opencv_cam: camera manager, starts/stops mp4 recording on arm/disarm</li> <li>bag_recorder_2: bag recorder, starts/stops ROS2 mcap on arm/disarm</li> <li>image_to_v4l2loopback_ros2: makes ROS2 image topic available for RTSP streaming</li> </ul> <p>Setup for additional perception capabilities such as SLAM, path planning, exploration, thermal image processing, and more are detailed in the Setup Guide.</p>"},{"location":"#data-flow","title":"Data Flow","text":"<pre><code>Frontend \u2500\u2500\u25b6 REST \u2500\u2500\u25b6 REST Server \u2500\u2500\u25b6 ROS 2 Topics \u2500\u2500\u25b6 ROS Nodes\n</code></pre>"},{"location":"#development","title":"Development","text":"<p>The ROS flight stack is modular, so e.g. you can swap out your own SLAM algorithm or path planner. Additional modules can be added as well. We'd love for someone to add reactive obstacle avoidance to the path manager!</p> <p>This site contains setup instructions, usage examples, architecture details, and more.</p> <p>\u27a1\ufe0f To get started, head to the Setup Guide.</p>"},{"location":"quickstart/","title":"Quick Start","text":""},{"location":"quickstart/#single-script-setup-sim","title":"Single Script Setup -- Sim","text":"<p>For workspace: desktop, laptop, etc. This is a support script to install, build, and launch everything at once.</p> <pre><code>mkdir ~/src\ncd ~/src\ngit clone https://github.com/robotics-88/open-drone-core.git\ncd open-drone-core\n./scripts/quickstart.sh\n</code></pre> <p>This brings up Gazebo, ArduCopter, ROS2 nodes, and the frontend, even if you start from a fresh Ubuntu install without ROS2. Should look like this when launching is complete:</p> <p></p>"},{"location":"quickstart/#test-setpoint-mission","title":"Test Setpoint Mission","text":"<p>Send your first setpoint mission from the frontend at http://127.0.0.1:8040/ by setting the IP in the Mission Panel to localhost and dropping a pin on the map. Watch the drone take off in Rviz or Gazebo.</p> <p></p>"},{"location":"quickstart/#single-script-setup-drone","title":"Single Script Setup -- Drone","text":"<p>For the drone's onboard Jetson. This will clone and install everything assuming a brand new Jetson, including installing ROS2, with the assumption we are running Ubuntu 22. <pre><code>cd\nmkdir src\ncd src\ngit clone https://github.com/robotics-88/open-drone-core.git\ncd open-drone-core\n./scripts/setup-drone.sh\nsource ~/.bashrc\n</code></pre></p>"},{"location":"analytics/","title":"Post-Flight Analytics","text":"<p>Note</p> <p>This section is a work in progress.</p> <p>The following pages explain how to run our open-goodfire-tools package, which provides lidar and video analytics for forest and fire data processing. This tool is independent of our flight stack, and can be used on any lidar and video datasets. However, drones using our flight stack do provide the relevant required inputs.</p>"},{"location":"analytics/#lidar-overview","title":"LiDAR Overview","text":"<p>Our LiDAR workflow provides the following data products from a .laz pointcloud:</p> <ul> <li> DEM tif</li> <li> Aspect tif</li> <li> Slope tif</li> <li> Base Canopy Height Model tif</li> <li> Tree ID LAS file</li> <li> Diameter at Breast Height csv</li> <li> Fuel volume tif (when before and after laz both available)</li> </ul>"},{"location":"analytics/#video-overview","title":"Video Overview","text":"<p>Our video workflow provides the following data products from a .mp4:</p> <ul> <li> Gaussian Splats</li> <li> PCD</li> </ul>"},{"location":"analytics/lidar/","title":"LiDAR","text":"<p>This package processes georegistered LiDAR in las or laz format into data for good fire, including:</p> <ul> <li> DEM tif</li> <li> Aspect tif</li> <li> Slope tif</li> <li> Base Canopy Height Model tif</li> <li> Tree ID LAS file</li> <li> Diameter at Breast Height csv</li> </ul>"},{"location":"analytics/lidar/#laz-pipeline","title":"LAZ Pipeline","text":"<p>Place your laz file/s in the folder <code>data/mydataset/input/before</code>. Use this folder regardless of whether there is a corresponding before/after fire dataset. However, if there is a before and after, put the after-fire dataset in <code>data/mydataset/input/after</code>. If this folder doesn't exist or is empty, the script will just run the analytics that don't require before and after.</p> <p>TODO: If there is a single laz file in <code>before/</code>, the script will segment it first into tiles. If there is a corresponding <code>after/</code>, the script will clip both datasets to the overlapping section and produce corresponding tiles.</p>"},{"location":"analytics/lidar/#setup","title":"Setup","text":""},{"location":"analytics/lidar/#clone","title":"Clone","text":"<pre><code>cd ~/src\ngit clone https://github.com/robotics-88/open-goodfire-tools.git\n</code></pre>"},{"location":"analytics/lidar/#install-dependencies-ubuntu-instructions","title":"Install dependencies (Ubuntu instructions)","text":"<pre><code>cd open-goodfire-tools/lidar\nsudo ./setup.sh\n</code></pre>"},{"location":"analytics/lidar/#usage","title":"Usage","text":"<p>Run the following on your data ('mydataset'):</p> <pre><code>cd open-goodfire-tools/lidar\nsource ../.env/bin/activate\npython process.py mydataset\n</code></pre> <p>To experiment with our test dataset, first download it using the provided script: <pre><code>cd open-goodfire-tools/lidar\n./scripts/download_sample.sh\n</code></pre> Then run:</p> <pre><code>cd open-goodfire-tools/lidar\nsource ../.env/bin/activate\npython process.py test\n</code></pre>"},{"location":"analytics/lidar/#outputs","title":"Outputs","text":"<p>We assume the starting point is a before and optional after laz. The test dataset before and after looks like:</p> <p></p> <p>Above: Example of \"before\" (left) and \"after\" (right) LiDAR point cloud visualizations from the test dataset.</p> <p>When before and after are both present, we can compute the fuel volume difference:</p> <p></p> <p>Above: Visualization of computed fuel volume difference between before and after LiDAR datasets.</p> <p>All other data can be computed with only the before pointcloud.</p> <p>We provide tree segmentation:</p> <p></p> <p>Above: Example of tree segmentation output from the LiDAR pipeline.</p> <p>For data layers provided by LANDFIRE, we fuse the high-resolution LiDAR data with the 30m resolution FlamMap layers from LANDFIRE. For example:</p> <p>Canopy base height</p> <p></p> <p>Above: Example of canopy base height output from the LiDAR pipeline.</p> <p>DEM:</p> <p></p> <p>Above: Example of Digital Elevation Model (DEM) output from the LiDAR pipeline.</p>"},{"location":"analytics/lidar/#troubleshooting","title":"Troubleshooting","text":""},{"location":"analytics/lidar/#flammap-downloads","title":"FlamMap downloads","text":"<p>FlamMap often returns errors or HTML, e.g. when undergoing maintenance or capacity issues. If you have issues with the script, test using the RLandfire package in terminal.</p> <p>Start an R terminal with: <pre><code>R\n</code></pre> Then: <pre><code>library(rlandfire)\nresp &lt;- landfireAPIv2(products = \"240EVC\",\n                    aoi = c(-105.40207, 40.11224, -105.23526, 40.19613),\n                    email = \"rlandfire@example.com\",\n                    verbose = FALSE)\n</code></pre> If this works, (TODO more assistance debugging our script). If this doesn't work, there is an issue on the LANDFIRE server side that is most likely resolved by waiting and trying again later.</p>"},{"location":"analytics/video/","title":"Video","text":"<p>The data product we generate from video is called a splat. This is a visually accurate 3D model useful for human interpretation. Future work is to update this to fuse LiDAR with video, which produces geometrically accurate 3D models that can then be a basis for further analytics.</p>"},{"location":"analytics/video/#setup","title":"Setup","text":""},{"location":"analytics/video/#install-dependencies-ubuntu-instructions","title":"Install dependencies (Ubuntu instructions)","text":"<p>Ensure submodules are checked out: <pre><code>cd open-goodfire-tools\ngit submodule update --init\n</code></pre> Then run setup: <pre><code>cd open-goodfire-tools/video\nsudo ./setup.sh\n</code></pre></p>"},{"location":"analytics/video/#gaussian-splat-pipeline","title":"Gaussian Splat Pipeline","text":"<p>Place your video named <code>&lt;dataset_name&gt;.mp4</code> in the folder <code>video/data/input</code>. Then run the following:</p> <pre><code>cd open-goodfire-tools/video\nsource ../.env/bin/activate\npython gsplat.py --dataset &lt;dataset_name&gt; -vv --sfm odm\n</code></pre>"},{"location":"development/","title":"Development","text":"<p>This flight stack is intended to simplify high-level autonomy R&amp;D. We have simple procedures for adding custom mission definitions, perception, and decision-making criteria for navigation. Each procedure is outlined in the following pages, but briefly:</p> <ul> <li>Mission: Active process that changes the flight behavior</li> <li>Perception: Passive process that can optionally be enabled during any mission</li> <li>Decision-making Criteria: Function to support optimal 3D navigation in complex environments</li> </ul>"},{"location":"development/decisionmaking/","title":"Decision-Making","text":"<p>The path manager includes a decision-making algorithm for modifying destination targets. Decisions are the result of a weighted average of user-defined criteria. The criteria can be anything that better serves your application. E.g., to modify destination targets in a subcanopy environment, the SAFETY criteria is used to push targets away from obstacles detected by LiDAR. And to improve ash tree finding, the ASH criterion is used to push targets toward areas more likely to include ash trees.</p> <p>Built-in criteria are:</p> <ul> <li>DIST: Mimimize distance to target</li> <li>INFO: Seek targets close to unexplored areas</li> <li>EFFICIENCY: Minimize distance between target and prior goal</li> <li>SAFETY: Maximize distance from target to nearest obstacle</li> <li>ASH: Seek targets where the terrain is a local valley (requires DEM)</li> </ul> <p>Each criteria requires a utility function. E.g., DIST uses Euclidean distance from the drone to the target as its utility function. This is not a perfect representation of the actual distance that will be traveled, just a prediction. We could have instead run the path planner, but fast is better than perfect. Utility functions should be fast, as they will be run on all destinations to identify the optimal next target. Right now, we produce 10 possible destinations, and all destinations are evaluated across all criteria.</p> <p>To add a new criterion:</p> <ol> <li>DECLARE: Add the criterion to the <code>DecisionMaker::Criteria</code> enum in <code>path_manager/explorer/decision_maker.h</code></li> <li> <p>LAUNCH: Add the criterion and its weight to the launch args list in <code>vehicle_launch/launch/opendrone.launch.py</code> in this line:</p> <pre><code>DeclareLaunchArgument('criteria', default_value='DIST,INFO,SAFETY,MYCRITERIA'),\nDeclareLaunchArgument('weights', default_value='0.2,0.1,0.6,MYWEIGHT')\n</code></pre> <p>Weights should sum to 1. They will be normalized if they do not, which may result in misaligned preferences.</p> </li> <li> <p>DEFINE: Add the criterion's corresponding utility function to the switch in <code>DecisionMaker::utilityFunction</code> in <code>path_manager/explorer/decision_maker.cpp</code>. If it is more complex than a single line of code, make a new method and call it from the switch. </p> <p>\ud83d\udea8 IMPORTANT: utility functions must be written such that high scores are better! Otherwise it will optimize for the opposite of your criterion.</p> </li> </ol>"},{"location":"development/missions/","title":"Missions","text":"<p>Active process that changes the flight behavior. E.g., a lawnmower flight pattern is an industry-standard mission. Trail-following is a somewhat more advanced mission.</p>"},{"location":"development/missions/#mission-specification","title":"Mission Specification","text":"<p>The mission file must be saved as <code>mission_name.json</code> in <code>task_manager/missions/</code>. Mission definition examples are provided.</p> <p>We provide 2 basic starter missions (setpoint, lawnmower), and 1 more advanced (trail_follow). The setpoint mission is <code>setpoint.json</code>:</p> <pre><code>{\n  \"mission\": {\n    \"name\": \"SETPOINT\"\n  },\n  \"requirements\": {\n    \"perception\": {\n      \"required\": [\"lidar_slam\"],\n      \"optional\": [\"obstacle_avoidance\"]\n    },\n    \"input_geometry\": \"point\"\n  }\n}\n</code></pre> <p>Note that the mission name must be unique, and must match the enum label in Task Manager. Also note the required versus optional perception. Technically, all perception modules can optionally be run with any mission. This field should only be used for perception that changes the mission behavior when enabled. Input geometry tells the frontend what to require before sending the mission. Options are <code>point</code> and <code>polygon</code>. Compare with the trail following mission, <code>trail_follow.json</code>:</p> <pre><code>{\n  \"mission\": {\n    \"name\": \"TRAIL_FOLLOW\"\n  },\n  \"requirements\": {\n    \"perception\": {\n      \"required\": [\"lidar_slam\", \n                  \"obstacle_avoidance\", \n                  \"trail_detection\"]\n    },\n    \"input_geometry\": \"\"\n  }\n}\n</code></pre>"},{"location":"development/missions/#task-manager-integration","title":"Task Manager Integration","text":"<p>Missions must be integrated into Task Manager so the state machine is aware of them. The frontend will be notified by Task Manager as long as the above-defined mission config file is present, as Task Manager searches that folder. However, even if sent by the frontend, TM won't do anything with this mission unless you do the following.</p> <ol> <li>Add mission name to TaskManager::MissionType enum in <code>task_manager/include/task_manager/task_manager.h</code> (must match the name of the config file! e.g. <code>setpoint.json</code> and <code>MissionType::SETPOINT</code>)</li> <li>Add switch for the new enum in TaskManager::switchMission() in <code>task_manager/src/task_manager.cpp</code></li> <li>Add mission method called within switch, defining the method as <code>TaskManager::handleMissionName</code> (if adding your own ROS package in the optional step below, this is where the service start or callback publisher should be sent from)</li> </ol>"},{"location":"development/missions/#optional-add-mission-capability-ros-package","title":"(Optional) Add Mission Capability ROS Package","text":"<p>Simple missions can be contained entirely within Task Manager methods. However, for anything beyond the most basic functionality, a new package should be created. There is significantly more flexibility with this approach. Your ROS2 package should provide a service or callback which is activated by the corresponding TM method. Beyond that, you must also:</p> <ol> <li>Add package to <code>open-drone-core/src/decco.repos</code></li> <li>Add package launch to <code>vehicle_launch/launch/opendrone.launch.py</code></li> </ol>"},{"location":"development/modules/","title":"Perception Modules","text":"<p>Passive process that can optionally be enabled during any mission. E.g., thermal hotspot ID could be active during setpoint or lawnmower missions. When modules are added following the instructions on this page, they will be available for toggling on/off for any given mission through our frontend's Mission Center (unless they are set as required by the mission in its config file).</p> <p><pre><code>{\n    \"lidar_slam\": {\n      \"node\": \"fastlio_mapping\",\n      \"hardware_required\": [\"lidar\"]\n    },\n    \"trail_detection\": {\n      \"node\": \"trail_follower_node\",\n      \"hardware_required\": [\"lidar\"]\n    },\n    \"thermal_fire\": {\n      \"node\": \"thermal_pipeline\",\n      \"hardware_required\": [\"thermal\"]\n    }\n}\n</code></pre> Note that hardware_required options can be any of the following: [\"lidar\", \"thermal\", \"rgb\", \"nir\"]</p> <p>If you add a new perception module, first add your module to the perception json in vehicle_launch (<code>vehicle_launch/config/perception.json</code>). Then add the following code snippets to your module.</p>"},{"location":"development/modules/#c","title":"C++","text":"<p>In your header, add: <pre><code>// Parameter handling for toggle on/off\nbool is_active_; // Set false here or at the start of your constructor!\nstd::shared_ptr&lt;rclcpp::ParameterEventHandler&gt; param_subscriber_;\nstd::shared_ptr&lt;rclcpp::ParameterCallbackHandle&gt; cb_handle_;\nrclcpp::TimerBase::SharedPtr param_monitor_timer_;\nvoid parameterCallback(const rclcpp::Parameter &amp;param);\nvoid startParamMonitoring();\n// End parameter handling\n</code></pre></p> <p>In your class file, add this to the end of your constructor:</p> <pre><code>is_active_ = false;\nparam_subscriber_ = std::make_shared&lt;rclcpp::ParameterEventHandler&gt;(this);\nstartParamMonitoring(); // Use timer to wait for task_manager to load perception registry\n</code></pre> <p>Then add these methods. Note this is the only code snippet where you need to modify with your own class and node names: <pre><code>void ClassName::parameterCallback(const rclcpp::Parameter &amp;param) {\n    is_active_ = param.as_bool();\n    RCLCPP_INFO(this-&gt;get_logger(), \"ClassName node active: %s\", is_active_ ? \"true\" : \"false\");\n}\n\nvoid ClassName::startParamMonitoring() {\n    param_monitor_timer_ = this-&gt;create_wall_timer(\n        std::chrono::seconds(1),\n        [this]() {\n            static bool callback_registered = false;\n\n            if (!callback_registered) {\n                try {\n                    cb_handle_ = param_subscriber_-&gt;add_parameter_callback(\n                        \"/task_manager/node_name/set_node_active\",\n                        std::bind(&amp;ClassName::parameterCallback, this, std::placeholders::_1),\n                        \"task_manager/task_manager\"\n                    );\n                    RCLCPP_INFO(this-&gt;get_logger(), \"\u2705 Parameter callback registered for task_manager:node_name/set_node_active\");\n                    callback_registered = true;\n                    param_monitor_timer_-&gt;cancel();  // stop retrying\n                } catch (const std::exception &amp;e) {\n                    RCLCPP_WARN(this-&gt;get_logger(), \"Waiting for task_manager param to become available: %s\", e.what());\n                }\n            }\n        });\n}\n</code></pre></p> <p>Finally, insert the flag in your node's process: <pre><code>// at the top of your primary method\nif (!is_active_) {\n    return;\n}\n</code></pre></p>"},{"location":"development/modules/#python","title":"Python","text":"<p>TODO</p>"},{"location":"setup/","title":"Setup Guide","text":"<p>The flight stack can be tested with physical hardware or entirely in simulation. For hardware setup, start here. For simulation setup, start here. If you are going to integrate the stack on a custom drone, we recommend starting with simulation setup to verify your configuration, then proceed to Custom.</p>"},{"location":"setup/#prerequisites","title":"Prerequisites","text":"<p>The machine where the flight stack runs, on drone or in sim on a computer, must be running Ubuntu 22.04. All dependent software including ROS2 will be installed with the setup script. To summarize, we require:</p> <ul> <li>Ubuntu 22.04</li> <li>ROS 2 Humble</li> <li><code>colcon</code>, <code>vcs</code>, <code>python3-pip</code></li> <li>Git</li> </ul> <p>Info</p> <p>Future work: upgrade to ROS2 Kilted.</p>"},{"location":"setup/#setup-overview","title":"Setup Overview","text":"<p>Open drone setup includes cloning the ROS2 workspace, setting up a sim environment, and launching for the first time. If using the frontend, you can proceed to launching your first setpoint mission through the map interface. If you are setting up a physical vehicle of your own, there is additional vehicle config setup.</p>"},{"location":"setup/#vehicle-configs","title":"Vehicle Configs","text":"<p>The system is built to automatically launch all perception features enabled by the types of sensors on your vehicle. For that to be possible, you must set up a config file as described here.</p>"},{"location":"setup/custom/","title":"Custom","text":"<p>Even if you are using a custom drone, it is best to first install all required packages per the Quickstart instructions. Once complete, next do the following 4 steps to configure the software for your custom drone.</p> <ol> <li> <p>Config</p> <p>For custom drone setup, the first step is to create a vehicle config file with all your sensors as described here.</p> </li> <li> <p>Sensors</p> <p>If you're adding a new sensor, you need to add a launch file in <code>vehicle_launch/launch/sensors</code>. This can simply launch another package's launch file, but it must match the name scheme in the config file you just created.</p> </li> <li> <p>Launch</p> <p>Test that your new config is launching as expected with:</p> <p><code>ros2 launch vehicle_launch opendrone.launch.py config_file:=myconfig.yaml</code></p> <p>If this works, update the systemd service in the next step so your config is the system default.</p> </li> <li> <p>Systemd</p> <p>The quickstart instructions set up a systemd service so the ROS nodes start on boot. Update the script to use your config file as a launch arg:</p> <pre><code>cd ~/src/open-drone-core\nnano run_drone.sh\n</code></pre> <p>Change the script on the config_file line:</p> <pre><code>#!/bin/bash\n\nsource /opt/ros/humble/setup.bash\nsource /home/$USER/src/open-drone-core/install/setup.bash\nsource /home/$USER/src/livox_ros_driver2/install/setup.bash\n\n# Get UTC date for logging\ndate=$(date -u '+%Y-%m-%d_%H-%M-%S')\ndata_directory=/home/$USER/r88_public/records/$date/\nmkdir -p $data_directory\n\nstdbuf -oL ros2 launch vehicle_launch opendrone.launch.py \\\n    config_file:=myconfig.yaml \\ # &lt;------ change this to your config \n    data_directory:=$data_directory $@ 2&gt;&amp;1 | tee $data_directory/distal_stdout_$date.log\n</code></pre> <p>Restart the service for the change to take effect: <pre><code>sudo systemctl restart drone.service\n</code></pre></p> </li> </ol> <p>That's it! </p> <p>If you've already set up the frontend on a machine that's on the same network as your vehicle, you should now be able to connect here.</p>"},{"location":"setup/drones/","title":"IRL Drones","text":"<p>The general architecture for running the system on a real drone is shown below.</p> <pre><code>flowchart LR\n    subgraph Drone\n        ROS[ROS 2]\n        REST[REST Server]\n    end\n\n    subgraph Frontend Host\n        FE_Server[Frontend Server]\n    end\n\n    subgraph Clients\n        Browser1[Client Browser]\n        Browser2[Client Browser]\n    end\n\n    ROS &lt;--&gt; REST\n    REST &lt;--&gt; FE_Server\n    FE_Server &lt;--&gt; Browser1\n    FE_Server &lt;--&gt; Browser2\n</code></pre> <p>The following pages outline the steps for setting up all of the above. First we present existing compatible drones, and a list of requirements to check if your own drone is compatible. Then we proceed to setup. Briefly, the procedure is to clone and install the flight stack, define your vehicle config (i.e., your flight controller and sensors and their positions), and finally install support software (optional but recommended for ease of use).</p>"},{"location":"setup/drones/config/","title":"Config","text":"<p>Vehicle config files are <code>yaml</code> files describing the flight controller and sensors on a given platform. These live in <code>vehicle_launch/config/vehicles/</code>. We provide the config files for the Robotics 88 drones, Decco and Ecco, but other drones require adding your own config.</p> <p>The format is:</p> <pre><code>drone_id: decco_001\nframe_id: base_link\nsimulated: false                                # true for sim, false for IRL\n\nflight_controller:\n  type: ardupilot                               # ardupilot or px4\n  fcu_url: /dev/cubeorange:57600                # recommend setting up a symlink such as here for cubeorange\n\nsensors:\n  camera_front:                                 # cameras must be named camera_*\n    type: mapir_rgn                             # this name must match the wrapper launch file, e.g. mapir_rgn.launch, which must live in vehicle_launch/launch/sensors\n    topic: /mapir_rgn/image_raw\n    frame: mapir_rgn\n    stream: camera1                             # optional arg, required for any camera that will be streamed to frontend\n    position: [0.094, 0.021, .057]              # with respect to base_link\n    orientation_rpy: [0.0, 0.0, 0.0]            # with respect to base_link\n\n  camera_thermal:                               # thermal cameras must include thermal in the name\n    type: seek_thermal                          # this name must match the wrapper launch file, e.g. seek_thermal.launch, which must live in vehicle_launch/launch/sensors\n    topic: /seek_thermal/image_raw\n    frame: seek_thermal\n    stream: camera2\n    stream_width: 320                           # optional arg for streaming at different resolution than default (640x480)\n    stream_height: 240                          # optional arg for streaming at different resolution than default (640x480)\n    position: [0.094, 0.021, .057]\n    orientation_rpy: [0.0, 0.0, 0.0]\n\n  lidar_top:                                    # at present we only support LiDAR-based SLAM, so lidar_top must be included to enable SLAM\n    type: mid360                                # this name must match the wrapper launch file, e.g. mid360.launch, which must live in vehicle_launch/launch/sensors\n    topic: /livox/lidar\n    frame: livox_frame\n    position: [0.0138761, 0.0, 0.132799]\n    orientation_rpy: [0.0, 0.3926991, 0.0]\n</code></pre>"},{"location":"setup/drones/config/#key-details","title":"Key Details","text":"<p>To highlight the key details:</p> <ul> <li>Cameras must be named following <code>camera_*</code> format</li> <li>Thermal cameras must additionally include <code>thermal</code> in the name</li> <li>SLAM will only launch when <code>lidar_top</code> is included in the sensor list (Visual SLAM is on the docket of features to add)</li> <li>Cameras that should be streamed to a frontend must include the <code>stream</code> arg which must be set to <code>cameraX</code></li> </ul>"},{"location":"setup/drones/flightstack/","title":"Flight Stack Setup","text":"<p>If you've confirmed your hardware is compatible, the next step is configuring core software. Note that Quickstart provides instructions on using the setup script for a drone, which installs everything required for the complete integrated system on a drone. Instructions repeated below.</p>"},{"location":"setup/drones/flightstack/#ros2-workspace","title":"ROS2 Workspace","text":"<p>This will clone everything assuming a brand new Jetson, including installing ROS2, with the assumption we are running Ubuntu 22. <pre><code>cd\nmkdir src\ncd src\ngit clone https://github.com/robotics-88/open-drone-core.git\ncd open-drone-core\n./scripts/setup-drone.sh\nsource ~/.bashrc\n</code></pre></p>"},{"location":"setup/drones/frontend/","title":"Frontend","text":"<p>We have provided a generic frontend for accessing drone status and sending missions. This is akin to something like QGroundControl, except that where QGC is targeted for manual and low-level drone control, this interface is targeted for high-level mission control.</p>"},{"location":"setup/drones/frontend/#rest-server","title":"REST Server","text":""},{"location":"setup/drones/frontend/#start-on-boot","title":"Start on Boot","text":"<p>Make this file <code>sudo nano /etc/systemd/system/drone-server.service</code> and paste in it: <pre><code>[Unit]\nDescription=Drone REST Server\nAfter=network.target\n\n[Service]\nUser=decco\nWorkingDirectory=/home/decco/src/open-drone-server\nExecStart=/bin/bash -c 'source /opt/ros/humble/setup.bash &amp;&amp; source .env/bin/activate &amp;&amp; python main.py'\nRestart=on-failure\nEnvironment=PYTHONUNBUFFERED=1\n\n[Install]\nWantedBy=multi-user.target\n</code></pre></p>"},{"location":"setup/drones/models/","title":"Compatible Drones","text":"<p>This flight stack is intended to be fully generic, able to integrate with any drone running Ubuntu and a Mavlink-based flight controller (PX4/ArduCopter agnostic). This page is a starting point for those looking for a drone or kit to purchase and setup with this software.</p> <p>Note</p> <p>Prices listed below are not necessarily up to date.</p>"},{"location":"setup/drones/models/#tested","title":"Tested","text":"<ul> <li>Drone Dojo: Raspberry Pi drone, ~$900<ul> <li>Caveat: For use with LiDAR and 3D mapping, you will likely need to replace the onboard computer with a Jetson Orin Nano and design your own 3D mount. Fine as is for camera-only models.</li> </ul> </li> <li>Robotics 88<ul> <li>TBD when this will be commercially available, we built our own model and are trying to decide what to do with it as the company is closing</li> </ul> </li> </ul>"},{"location":"setup/drones/models/#untested","title":"Untested","text":"<p>Untested drones, but have the right hardware in theory:</p> <ul> <li>Indro Robotics: R&amp;D Drone with Jetson Xavier NX, $$?</li> <li>Momentum Drones: R&amp;D Drone with Jetson Orin NX, $7k</li> </ul>"},{"location":"setup/drones/support/","title":"Useful Support Software","text":"<p>These are support libraries we run on IRL drones to make life easier. E.g., simplify ssh, file access, video streaming, etc. Note that all of these are set up by default if you use the quickstart script. If instead you want to manually install them, instructions follow.</p>"},{"location":"setup/drones/support/#mdns","title":"mDNS","text":"<p>This will make it so on any connected network, your drone hostname shows up as <code>drone.local</code> instead of requiring a specific IP. <pre><code>sudo apt update\nsudo apt install avahi-daemon avahi-utils\nsudo hostnamectl set-hostname drone\n</code></pre> And so that it starts on boot: <pre><code>sudo systemctl enable avahi-daemon\nsudo systemctl start avahi-daemon\n</code></pre> From here on, instructions will assume the drone can be pinged/ssh'd at <code>decco@drone.local</code>. If you skip this step, replace this with the usual <code>decco@ipaddr</code>.</p> <p>Tip</p> <p>If you plan to run the frontend and want to access it easily from multiple devices, you can do the same thing on your laptop, eg name it <code>frontend.local</code> so anyone can pull up the drone interface.</p>"},{"location":"setup/drones/support/#video-stream","title":"Video stream","text":"<p>We use the mediamtx video server so we can easily switch between RTSP, WebRTC, HLS, etc.</p> <p>Tip</p> <p>If you use the open drone frontend, after setup, this stream will be automatically available in the drone interface.</p> <p>Download and extract this release: <pre><code>cd ~/src\nmkdir video\ncd video\nwget https://github.com/bluenviron/mediamtx/releases/download/v1.8.1/mediamtx_v1.8.1_linux_arm64v8.tar.gz\ntar -xf mediamtx_v1.8.1_linux_arm64v8.tar.gz\n</code></pre></p> <p>Change the RTSP params: <pre><code>cd ~/src/video\nnano mediamtx.yml\n</code></pre></p> <p>Scroll to the end and change the <code>paths</code> section to read: <pre><code>paths:\n  camera:\n    runOnDemand: ffmpeg -f v4l2 -i my_video_device -pix_fmt yuv420p -c:v libx264 -preset ultrafast -tune zerolatency -b:v 1M -f rtsp rtsp://localhost:$RTSP_PORT/$MTX_PATH\n    runOnDemandRestart: yes\n</code></pre> Replace <code>my_video_device</code> as needed e.g. with <code>/dev/video0/</code> or preferably, a symlink. It's better if the device is symlinked in case video devices get reordered on boot. Default devices (mapir, immervision, and seek cameras) get symlinks created by running the quickstart script. If adding your own device, create a symlink by:</p> <p><pre><code>lsusb\n</code></pre> Output should be e.g. <pre><code>Bus 001 Device 004: ID 0603:8612 Novatek Microelectronics Corp. MAPIR\n</code></pre> Make a file <code>drone-99.rules</code> and copy this into it: <pre><code>ACTION==\"add\", SUBSYSTEM==\"video4linux\", ATTRS{idVendor}==\"0603\", ATTRS{idProduct}==\"8612\", ATTR{index}==\"0\", SYMLINK+=\"mapir\"\n</code></pre> The symlink arg can be whatever you choose. Then create the symlink with: <pre><code>sudo cp drone-99.rules /etc/udev/rules.d/\nsudo udevadm control --reload-rules &amp;&amp; sudo udevadm trigger\n</code></pre></p> <p>To test if the stream is working, run with: <pre><code>./mediamtx\n</code></pre></p> <p>After confirming, set up a systemd service so the stream starts automatically on boot.</p>"},{"location":"setup/drones/support/#start-on-boot","title":"Start on Boot","text":"<p><pre><code>cd ~/src/video\nsudo cp mediamtx /usr/local/bin/\nsudo cp mediamtx.yml /usr/local/etc/\nsudo nano /etc/systemd/system/mediamtx.service\n</code></pre> In the service file, paste: <pre><code>[Unit]\nDescription=Drone Video Stream\nWants=network.target\n[Service]\nExecStart=/usr/local/bin/mediamtx /usr/local/etc/mediamtx.yml\n[Install]\nWantedBy=multi-user.target\n</code></pre></p> <p>Now enable the service: <pre><code>sudo systemctl daemon-reload\nsudo systemctl enable mediamtx.service\nsudo systemctl start mediamtx.service\n</code></pre></p>"},{"location":"setup/drones/support/#view-stream","title":"View Stream","text":"<p>The easiest place to test is in our frontend, which automatically connects and displays the stream. For an alternate, you can use QGroundControl. In Application Settings, enable the video stream and set the RTSP stream to <code>rtsp://drone.local:8554/camera</code> like so: </p>"},{"location":"setup/drones/support/#file-manager","title":"File Manager","text":"<p>We provide a file manager service that enables easy file transfer from flight data (videos, pointclouds, logs, ROS2 mcaps). This is a service that runs on the drone but can be accessed through the browser on a device on the same network. Setup using the standalone script:</p> <pre><code>cd ~/src/open-drone-core\n./scripts/steps/07-file-manager.sh\n</code></pre> <p>The files are then accessible at <code>http://drone.local:9999</code>. It should look like: </p>"},{"location":"setup/simulation/","title":"Sim Drones","text":"<p>The flight stack can be tested in sim using either AirSim or Gazebo. It is highly recommended to test any new changes in sim prior to flight on a real drone! The following pages detail sim setup for each. Note that Quickstart provides instructions on using the setup script for a workspace, which installs everything required for Gazebo sim. If you ran that and don't need AirSim, no need to do anything further for sim setup! However AirSim requires manually getting access to Unreal Engine so proceed to AirSim for further details on that, or if you wish to do a manual workspace setup, read on.</p> <p>Info</p> <p>Future work, add details on testing with PX4 in sim.</p> <p>To begin, the first step is install ArduPilot.</p> <p>Then, the setup order depends on which environment you plan to use. If you want to test both, start with AirSim as it has preliminary setup requirements.</p>"},{"location":"setup/simulation/airsim/","title":"AirSim","text":"<p>AirSim is an Unreal plugin developed by Microsoft but no longer maintained, so don't use the Microsoft version. We are now using our own fork of Colosseum because when we last checked theirs didn't work on Ubuntu 22 (TODO check again).</p>"},{"location":"setup/simulation/airsim/#unreal-setup","title":"Unreal Setup","text":"<ol> <li>Download and extract Unreal 5.4 to <code>~/src</code></li> <li> <p>Build:</p> <pre><code>cd ~/src/UnrealEngine-5.4.0-release\n./Setup.sh\n./GenerateProjectFiles.sh\nmake\n</code></pre> </li> <li> <p>If asked during Setup to register Unreal file types, say yes. Upon completion of the script, the \u201cSUCCESS\u201d message should print.</p> </li> <li>*Unclear if this is always required, but<ol> <li>make sure you\u2019re using an NVIDIA driver, you\u2019ll get a warning about Vulkan driver missing if not;</li> <li>if Unreal opens but the project fails, or if you get an error about incompatible versions/open with null source code, remove the args \u201c-NoEngineChanges -NoHotReloadFromIDE\u201d from DesktopPlatformBase.cpp, then make again</li> </ol> </li> </ol>"},{"location":"setup/simulation/airsim/#colosseum-setup","title":"Colosseum Setup","text":"<p>First add libs for vulkan.</p> <pre><code>wget -qO- https://packages.lunarg.com/lunarg-signing-key-pub.asc | sudo tee /etc/apt/trusted.gpg.d/lunarg.asc\nsudo wget -qO /etc/apt/sources.list.d/lunarg-vulkan-jammy.list http://packages.lunarg.com/vulkan/lunarg-vulkan-jammy.list\nsudo apt update\n</code></pre> <p>Then for libunwind:</p> <pre><code>sudo apt-get update\nsudo apt-get install libunwind-dev\n</code></pre> <p>Then:</p> <pre><code>cd ~/src\ngit clone git@github.com:robotics-88/Colosseum.git\ncd Colosseum\ngit checkout r88_5.2\n./setup.sh\n./build.sh\n</code></pre> <p>+</p> <p>Add to <code>~/.bashrc</code>:</p> <pre><code>export AIRSIM_DIR=\"/home/$USER/src/Colosseum\"\nsource ~/.bashrc\n</code></pre>"},{"location":"setup/simulation/airsim/#ros2","title":"ROS2","text":"<p>Our AirSim ROS2 wrapper depends on finding Colosseum/AirSim, so if you jump to this section the build will fail without the above. Assuming the prior steps are complete:</p>"},{"location":"setup/simulation/airsim/#clone-and-build","title":"Clone and Build","text":"<pre><code>cd ~/src\ngit clone https://github.com/robotics-88/open-drone-core.git\ncd open-drone-core\n./scripts/setup_workspace.sh\ncolcon build\n</code></pre>"},{"location":"setup/simulation/airsim/#launch","title":"Launch","text":"<p>Start Unreal: <pre><code>cd ~/src/Unreal\n./Engine/Binaries/Linux/UnrealEditor\n</code></pre> If running for the first time, you'll need to use the project browser to locate the R88 world. If you check \"open last project on start,\" then next time you run this terminal command, it will take you straight to the sim.</p> <p>Start ArduPilot: <pre><code>cd ~/src/r88_ardupilot\n./run_airsim.sh\n</code></pre></p> <p>In Unreal, press play (shortcut <code>Alt + P</code>).</p> <p>Run the flight stack with: <pre><code>cd open-drone-core\n./run_sim.sh do_airsim:=true do_gazebo:=false\n</code></pre></p> <p>Note</p> <p>Note, when you want to stop the sim, press stop in Unreal (shortcut ESC) BEFORE <code>Ctrl+C</code> in ArduPilot, or the window will freeze.</p>"},{"location":"setup/simulation/ardupilot/","title":"ArduPilot","text":"<p>We provide a custom fork of ArduPilot with tuned flight params built-in for our drones (real and sim) and support scripts.</p>"},{"location":"setup/simulation/ardupilot/#install","title":"Install","text":"<pre><code>cd ~/src\ngit clone --recurse-submodules git@github.com:robotics-88/r88_ardupilot.git\ncd r88_ardupilot\nTools/environment_install/install-prereqs-ubuntu.sh -y\nexport PATH=$PATH:$HOME/src/r88_ardupilot/Tools/autotest\nexport PATH=/usr/lib/ccache:$PATH\n. ~/.profile\n</code></pre> <p>Note</p> <p>For a long time, ArduPilot tools script installs a conflicting version of empy (conflicts with mavros). If you get em/empy errors, fix it with <code>pip install empy==3.3.4</code>. Recently it seems to install a conflicting version of setuptools. Fix it with <code>pip install 'setuptools&lt;66' --user</code>.</p>"},{"location":"setup/simulation/ardupilot/#usage","title":"Usage","text":"<p>To use in sim, run this for AirSim: <pre><code>cd r88_ardupilot\n./run_airsim.sh\n</code></pre></p> <p>And this for Gazebo: <pre><code>cd r88_ardupilot\n./run_gazebo.sh\n</code></pre> Until the corresponding sim environment is setup and running, this script will fail to find a drone to connect to. Proceed next to the desired sim setup.</p>"},{"location":"setup/simulation/gazebo/","title":"Gazebo","text":"<p>If you already ran the quickstart script, Gazebo sim should be ready to run, and you can jump to Launch below.</p>"},{"location":"setup/simulation/gazebo/#launch","title":"Launch","text":""},{"location":"setup/simulation/gazebo/#1-launch-gazebo","title":"1. Launch Gazebo","text":"<p>In a new terminal, start Gazebo with: <pre><code>gz sim -v4 -r r88.sdf\n</code></pre></p>"},{"location":"setup/simulation/gazebo/#2-launch-ardupilot","title":"2. Launch ArduPilot","text":"<p>Start the corresponding ArduPilot with: <pre><code>cd ~/src/r88_ardupilot\n./run_gazebo.sh\n</code></pre></p>"},{"location":"setup/simulation/gazebo/#3-launch-ros","title":"3. Launch ROS","text":"<p>Finally, start the ROS nodes. Gazebo is the default environment so no launch args are required. Run with: <pre><code>cd open-drone-core\n./run_sim.sh\n</code></pre></p> <p>TODO screenshots</p>"},{"location":"system/","title":"System","text":"<p>This page details all currently available ROS2 nodes, both core and optional. The main node in the system is Task Manager. This contains the state machine and the comms link to the REST API. The entire system architecture is detailed below, and the state machine is further detailed in State Machine.</p>"},{"location":"system/#core-ros2-nodes","title":"Core ROS2 Nodes","text":""},{"location":"system/#mission-flight-management","title":"Mission &amp; Flight Management","text":"<p>Core, required nodes.</p> <ul> <li>task-manager: state machine, receives and processes mission, manages flight controller params<ul> <li>This is where you add new missions</li> </ul> </li> <li>vehicle-launch: launch and vehicle config management<ul> <li>This is where you set up new vehicle configs or change perception modules</li> </ul> </li> <li>path_manager: takes task manager mission outputs and converts those goals to executable MAVROS setpoints (can be used with or without SLAM &amp;path planner)<ul> <li>This is where you get to decide how smart the drone really is, at minimum by enabling path planning, up to enabling full decision-making about what constitutes a good destination</li> </ul> </li> </ul>"},{"location":"system/#data-management","title":"Data Management","text":"<p>Handles generic cameras, video streaming, and data recording.</p> <ul> <li>opencv_cam: camera manager, starts/stops mp4 recording on arm/disarm, uses Nvidia hardware encoding if available</li> <li>bag_recorder_2: bag recorder, starts/stops ROS2 mcap on arm/disarm</li> <li>image_to_v4l2loopback_ros2: makes ROS2 image topic available for RTSP streaming</li> </ul>"},{"location":"system/#optional-nodes","title":"Optional Nodes","text":""},{"location":"system/#mapping","title":"Mapping","text":"<ul> <li>fast-lio2: SLAM for LiDAR<ul> <li>Works with Livox and Velodyne/Ouster types </li> <li>Strongly recommend Livox for cluttered environment, Velodyne/Ouster proved insufficient for detecting small twigs and are so much more expensive and heavy -- Livox is awesome</li> </ul> </li> <li>path-planner: path planning for LiDAR SLAM registered pointclouds</li> <li>pcl-analysis: pointcloud aggregation and filtering, saves .laz file at flight conclusion</li> </ul>"},{"location":"system/#perception","title":"Perception","text":"<ul> <li>thermal-pipeline: analyzes thermal imagery for fire perimeter and hot spots (with optional NIR fusion to reduce false positives)</li> <li>image-segment: (TODO make public) runs ML segmentation model on a ROS2 image topic</li> <li>pcl-analysis: (TODO separate this into another node?) detects powerlines and vegetation encroachment</li> </ul>"},{"location":"system/#sensors","title":"Sensors","text":"<p>Sensor specific ROS wrappers:</p> <ul> <li>Livox<ul> <li>Livox-SDK2: Livox SDK</li> <li>livox-ros-driver2: Livox ROS wrapper (ROS1/2)</li> </ul> </li> <li>seek-thermal: ROS2 wrapper for this thermal camera</li> <li>i2c_temperature: reads sensor data from this</li> </ul>"},{"location":"system/dataflow/","title":"Data Flow","text":"<p>Rather than look  at all data flowing through the system at once, it's easier to look at diagrams for specific data flows. On this page we provide diagrams for the 2 most critical data types, historically: navigation points and pointclouds.</p>"},{"location":"system/dataflow/#navigation-points","title":"Navigation Points","text":"<p>We use 3 different types of navigation points: Goals, targets, and setpoints. Each indicates increasing granularity. For example, a goal could be a pin dropped on the frontend map (called SETPOINT mission in the interface, maybe we should rename it). Then short-range targets are created in path manager as subgoals. This is because path planning and obstacle avoidance is more effective on short distances. This subgoal could be sent directly to path planner, but if explorer is enabled, first path manager will try to find a destination near the subgoal but far from obstacles (or whatever other decision-making criteria you implement, see Decision-Making). Either way, the subgoal is the target for the path planner. When path manager receives the resulting path, it breaks that down into even smaller MAVROS-ingestible setpoints, called such because the MAVROS topic is <code>/mavros/setpoint_raw/local</code>.</p> <pre><code>flowchart TD\n    subgraph Frontend\n        A[User Input / Mission Upload]\n    end\n\n    subgraph \"REST Server\"\n        B[Convert to ROS JSON string]\n    end\n\n    subgraph \"Task Manager\"\n        C[State machine\n        Mission prep]\n    end\n\n    subgraph \"Path Manager\"\n        E[Breakdown subgoals\n        Terrain adjust\n        Path into MAVROS setpoint]\n\n\n        subgraph Explorer\n            D[Find open space\n            Select best nearby target]\n        end\n    end\n\n    subgraph MAVROS\n        F[Setpoint execution]\n    end\n\n    subgraph \"Path Planner\"\n        G[Find path]\n    end\n\n    A --&gt;|mission| B\n    B --&gt;|mission| C\n    C --&gt;|goal| E\n    E --&gt;|goal| D\n    D --&gt;|clear space target| E\n    G --&gt;|path to target| E\n    E --&gt;|target| G\n    E --&gt;|setpoint| F\n\n</code></pre> <p>For example, the goal set for the mission could be: </p> <p>Then the path manager breaks this into subgoals which are sent to the path planner. In this image, explorer manager was enabled, so it produced 10 possible destinations (unselected options in green). The selected target is the pink ball. This was sent to the path planner, resulting in the green path. The path is then converted into MAVROS setpoints, as indicated by the red arrow just next to the drone (indicated by the TF stack) here. </p>"},{"location":"system/dataflow/#pointclouds","title":"Pointclouds","text":"<p>The pointcloud goes through a number of transformations, and almost every downstream perception node should be using the final pointcloud from the dataflow, called <code>/cloud_aggregated</code>. This is the cloud used by path planning, powerline detection, and trail following.</p> <pre><code>flowchart TD\n    subgraph Drone\n        Livox[Livox]\n        Jetson[Jetson Orin Nano]\n    end\n\n    subgraph Jetson\n        Livox_wrapper[Livox ROS Wrapper]\n        PCL[PCL Analysis]\n        SLAM[Fast LIO SLAM]\n        TaskMgr[Task Manager]\n        Perception[Multiple ROS2 perception nodes]\n    end\n\n    Livox --&gt;|raw data|Jetson\n    Livox_wrapper --&gt;|raw livox custom pointcloud|SLAM\n    SLAM --&gt;|livox frame registered Ros2 Pointcloud2|TaskMgr\n    TaskMgr --&gt;|map frame registered Ros2 Pointcloud2|PCL\n    PCL --&gt;|aggregated and cleaned Ros2 Pointcloud2|Perception\n</code></pre>"},{"location":"system/missions/","title":"Example Missions","text":"<p>Currently, the flight stack offers the following built-in missions, provided the correct hardware is available:</p> <p>Note</p> <p>TODO add screenshots of each in flight IRL</p>"},{"location":"system/missions/#goal-point","title":"Goal Point","text":"<p>Drop a point on the frontend map, and send a setpoint. If no DEM is included, no altitude adjustments are made. With DEM, the drone will stay within a predefined altitude volume AGL.</p>"},{"location":"system/missions/#powerline-following","title":"Powerline Following","text":"<p>Detect powerlines in LiDAR and navigate along them, avoiding and mapping vegetation along the line.</p> <p>Note</p> <p>Work in progress. Currently detects but does not send waypoints to follow.</p>"},{"location":"system/missions/#trail-following","title":"Trail Following","text":"<p>Detect trails in LiDAR and navigate along them, avoiding and mapping vegetation.</p>"},{"location":"system/missions/#smart-lawnmower","title":"Smart Lawnmower","text":"<p>This is an extension of the familiar lawnmower pattern, but with decision-making that enable the drone to adapt the path based on real-time conditions, e.g., to adjust for obstacles or cluttered environments. Any custom criteria can be added to enhance decision-making based on application-specific context. See the decision-making development guide for further instructions on adding your own.</p>"},{"location":"system/statemachine/","title":"Task Manager: State Machine","text":"<p>The primary control loop is the state machine in the task manager node. This state machine is described in the diagram below:</p> <pre><code>stateDiagram\n   direction TB\n\n   accTitle: State machine diagram for Open Drone Core\n\n   classDef system fill:var(--md-default-bg-color),stroke:#555,color:var(--md-default-fg-color)\n\n   [*] --&gt; INITIALIZING:::system\n   INITIALIZING --&gt; PREFLIGHT_CHECKS:::system\n   PREFLIGHT_CHECKS--&gt; READY:::system : if ready to arm\n   READY --&gt; PREFLIGHT_CHECKS : if not ready to arm\n\n   READY --&gt; MANUAL : on pilot takes off\n   MANUAL --&gt; COMPLETE:::system : on arm -&gt; disarm\n   COMPLETE --&gt; PREFLIGHT_CHECKS\n\n   READY --&gt; TAKING_OFF : on accepted mission\n   TAKING_OFF --&gt; MISSION : on altitude reached\n   MISSION --&gt; MISSION : on new mission accepted\n   MISSION --&gt; RTL_88 : on mission complete\n\n   RTL_88 --&gt; LANDING : on reached home\n   LANDING --&gt; COMPLETE : on arm -&gt; disarm\n</code></pre>"},{"location":"system/statemachine/#states","title":"States","text":"<p>States are defined by the <code>TaskManager::Task</code> enum and include the following:</p> <ul> <li> <p>INITIALIZING    The system is powering up and performing initial setup.</p> </li> <li> <p>PREFLIGHT_CHECKS    Pre-flight checks are performed by the onboard flight controller to ensure the system is ready for operation.</p> </li> <li> <p>READY    The system is ready to arm and begin flight operations.</p> </li> <li> <p>MANUAL_FLIGHT    The drone is under direct manual control by the pilot.</p> </li> <li> <p>PAUSE    The mission or flight is temporarily paused by the user.</p> </li> <li> <p>MISSION    The drone is executing an autonomous mission.</p> </li> <li> <p>RTL_88    Return-to-launch sequence is active (RTL). RTL_88 uses the obstacle-aware path planner and DEM when available, in contrast to the PX4/ArduPilot RTL methods.</p> </li> <li> <p>TAKING_OFF    The drone is in the process of taking off.</p> </li> <li> <p>LANDING    The drone is landing.</p> </li> <li> <p>FAILSAFE_LANDING    The drone is performing a failsafe landing due to an error or safety trigger.</p> </li> <li> <p>COMPLETE    The flight or mission has ended, and the system is disarmed.</p> </li> </ul>"},{"location":"system/statemachine/#events","title":"Events","text":"<p>Events cause the state machine to change states.</p> <ul> <li> <p>FLIGHT CONTROLLER CHECKS PASS/FAIL    The onboard flight controller checks pass, then the state moves to READY. If they fail, the state is PREFLIGHT_CHECKS.</p> </li> <li> <p>FRONTEND MISSION RECEIVED    If a mission is received and all the correct Task Manager integration is complete, MISSION state is activated.</p> </li> <li> <p>AUTONOMY ALTITUDE REACHED    Missions can only be activated after takeoff once the user-defined altitude volume is reached.</p> </li> <li> <p>PILOT TAKEOFF    The drone detects takeoff without a mission, not in guided mode.</p> </li> <li> <p>FRONTEND EMERGENCY BUTTON    The 3 frontend emergency buttons result in 3 separate possible states: PAUSE, RTL_88, and FAILSAFE_LANDING.</p> </li> <li> <p>MISSION COMPLETE    On completing the mission (e.g., setpoint reached, lawnmower points all completed), the drone enters hover mode and sets state to READY.</p> </li> <li> <p>AUTOMATIC FAILSAFE TRIGGERED    Any of the failsafes can be automatically detected and interrupt a lower priority state, moving to FAILSAFE_LANDING.</p> </li> </ul>"}]}